{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Value & Policy Iteration from scratch\n",
        "\n",
        "In this notebook I'll try to implement a simple example of value and policy iteration from scratch in Python. The only import we'll use it numpy."
      ],
      "metadata": {
        "id": "w_aABFZbunk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "HGLCaUms9nou"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MDP\n",
        "\n",
        "In order to do value- and policy iteration, we'll want to model the MDP. In this case, our \"world\" will be a 2D grid, where each cell has a reward value.\n",
        "\n",
        "To make life easier, I'll create a wrapper around the numpy array that contains the data, to introduce some convenient functions that simplify the code."
      ],
      "metadata": {
        "id": "5DBgZJT39bUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Grid:\n",
        "    def __init__(self, matrix):\n",
        "        self.matrix = np.array(matrix)\n",
        "\n",
        "    def get_val(self, state):\n",
        "        r, c = self.coord_2_index(state)\n",
        "        return self.matrix[r, c]\n",
        "\n",
        "    def set_val(self, state, value):\n",
        "        r, c = self.coord_2_index(state)\n",
        "        self.matrix[r, c] = value\n",
        "\n",
        "    def coord_2_index(self, state):\n",
        "        x, y = state\n",
        "        return len(self.matrix) - y, x - 1\n",
        "\n",
        "    def equals(self, grid):\n",
        "        return np.array_equal(self.matrix, grid.matrix)\n",
        "\n",
        "    def display(self):\n",
        "        print(self.matrix)"
      ],
      "metadata": {
        "id": "79PoYwj172XQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to perform the updates using the Bellman equations, I'll create another class modelling the MDP."
      ],
      "metadata": {
        "id": "wSEMCjQ2-KT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Mdp:\n",
        "    def __init__(self, states, actions, gamma):\n",
        "        self.states = states\n",
        "        self.actions = actions\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def T(s1, a, s2):\n",
        "        pass\n",
        "\n",
        "    def R(s1, a, s2):\n",
        "        pass"
      ],
      "metadata": {
        "id": "DYWRXjL674Lu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our grid-world example has a few specific transitions (NESW) and a simple reward function. So I'll create a new class that can model a grid-world of any size provided by the user."
      ],
      "metadata": {
        "id": "LP-1SRdK-pkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MdpGrid(Mdp):\n",
        "    def __init__(self, grid, terminal_states, gamma):\n",
        "        self.grid = grid\n",
        "        self.terminal_states = terminal_states\n",
        "        states = [(x, y) for x in range(1, len(self.grid.matrix) + 1) for y in range(1, len(self.grid.matrix[0]) + 1)]\n",
        "        actions = [\"N\", \"E\", \"S\", \"W\"]\n",
        "\n",
        "        super().__init__(states, actions, gamma)\n",
        "\n",
        "    def T(self, s1, a, s2):\n",
        "        if s1 in self.terminal_states:\n",
        "            return 0\n",
        "\n",
        "        transitions = {\n",
        "            \"N\": (0, 1),\n",
        "            \"E\": (1, 0),\n",
        "            \"S\": (0, -1),\n",
        "            \"W\": (-1, 0),\n",
        "        }\n",
        "        x, y = s1\n",
        "        x2, y2 = s2\n",
        "        dx, dy = transitions[a]  # Deterministic actions assumed\n",
        "        return int((x + dx, y + dy) == s2)\n",
        "\n",
        "    def R(self, s1, a, s2):\n",
        "        return self.grid.get_val(s2)"
      ],
      "metadata": {
        "id": "t-4swrRt-pBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we'll try experiment with a simple `4x4` grid."
      ],
      "metadata": {
        "id": "6B-8HlbR8Cm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MdpGrid4x4(MdpGrid):\n",
        "    def __init__(self, gamma):\n",
        "        matrix = [\n",
        "            [0, 0, 0, 1],\n",
        "            [0, 0, 0, -1],\n",
        "            [0, 0, 0, 0],\n",
        "            [0, 0, 0, 0],\n",
        "        ]\n",
        "        grid = Grid(matrix)\n",
        "        super().__init__(grid, [(4, 4), (4, 3)], gamma)"
      ],
      "metadata": {
        "id": "dRo-gZL676WF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Value Iteration\n",
        "\n",
        "$$\n",
        "V_{k+1}(s) \\leftarrow \\max_a \\sum_{s'} T(s, a, s') \\left[ R(s, a, s') + \\gamma V_k(s') \\right]\n",
        "$$\n"
      ],
      "metadata": {
        "id": "BlujORDx8Rap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ValueIteration():\n",
        "    def __init__(self, mdp):\n",
        "        self.mdp = mdp\n",
        "\n",
        "    def V(self, s, k):\n",
        "        if k <= 0: return 0\n",
        "\n",
        "        values = []\n",
        "\n",
        "        for s2 in self.mdp.states:\n",
        "            for a in self.mdp.actions:\n",
        "                # Calculate components\n",
        "                t = self.mdp.T(s, a, s2)\n",
        "                r = self.mdp.R(s, a, s2)\n",
        "                v2 = self.V(s2, k - 1) if t > 0 else 0\n",
        "\n",
        "                # Insert components into formula\n",
        "                v = t * (r + self.mdp.gamma * v2)\n",
        "\n",
        "                # Save to results\n",
        "                values.append(v)\n",
        "\n",
        "        return max(values)\n",
        "\n",
        "    def run(self, k):\n",
        "        shape = self.mdp.grid.matrix.shape\n",
        "        values = Grid(np.zeros(shape))\n",
        "\n",
        "        for s in self.mdp.states:\n",
        "            values.set_val(s, np.around(self.V(s,k), 2))\n",
        "\n",
        "        return values"
      ],
      "metadata": {
        "id": "oLmwn5yh8S1P"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy Iteration\n",
        "\n",
        "$$\n",
        "V^{\\pi_{i}}_{k+1}(s) \\leftarrow \\sum_{s'} T(s, \\pi_{i}(s), s') \\left[ R(s, \\pi_{i}(s), s') + \\gamma V^{\\pi_{i}}_{k}(s') \\right]\n",
        "$$"
      ],
      "metadata": {
        "id": "Ot-rQXu_8F5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PolicyIteration():\n",
        "    def __init__(self, mdp):\n",
        "        self.mdp = mdp\n",
        "\n",
        "    def policy_evaluation(self, s, pi, k):\n",
        "        if k <= 0:\n",
        "            return 0\n",
        "        value_sum = 0\n",
        "        for s2 in self.mdp.states:\n",
        "            a = pi.get_val(s)\n",
        "            t = self.mdp.T(s, a, s2)\n",
        "            r = self.mdp.R(s, a, s2)\n",
        "            v2 = self.policy_evaluation(s2, pi, k - 1) if t > 0 else 0\n",
        "            value_sum += t * (r + self.mdp.gamma * v2)\n",
        "        return value_sum\n",
        "\n",
        "    def policy_improvement(self, pi, eval_k):\n",
        "        new_pi = Grid(pi.matrix.copy())\n",
        "        for s in self.mdp.states:\n",
        "            action_values = []\n",
        "            for a in self.mdp.actions:\n",
        "                pi_copy = Grid(new_pi.matrix.copy())\n",
        "                pi_copy.set_val(s, a)\n",
        "                v = self.policy_evaluation(s, pi_copy, eval_k)\n",
        "                action_values.append((v, a))\n",
        "\n",
        "            best_action = max(action_values, key=lambda x: x[0])[1]\n",
        "            new_pi.set_val(s, best_action)\n",
        "\n",
        "        return new_pi\n",
        "\n",
        "    def run(self, pi, policy_iterations, value_iterations):\n",
        "        iter_pi = pi\n",
        "        for i in range(policy_iterations):\n",
        "            new_pi = self.policy_improvement(iter_pi, value_iterations)\n",
        "            if new_pi.equals(iter_pi):\n",
        "                break\n",
        "            iter_pi = new_pi\n",
        "        return iter_pi"
      ],
      "metadata": {
        "id": "o6dLPQqp79lh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Examples\n",
        "\n",
        "Let's initialize a new grid world and run policy and value iteration on both of them to see the results."
      ],
      "metadata": {
        "id": "gonvuooc8H_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mdp_4x4 = MdpGrid4x4(0.95)"
      ],
      "metadata": {
        "id": "P_aJsv5c7-3n"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "value_iter = ValueIteration(mdp_4x4)\n",
        "\n",
        "resulting_values = value_iter.run(5)\n",
        "resulting_values.display()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzbbB3DB-LiZ",
        "outputId": "1a832c6b-5f30-4a6c-81f3-b82ca3499565"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.9  0.95 1.   0.  ]\n",
            " [0.86 0.9  0.95 0.  ]\n",
            " [0.81 0.86 0.9  0.86]\n",
            " [0.   0.81 0.86 0.81]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "policy_iter = PolicyIteration(mdp_4x4)\n",
        "\n",
        "my_pi_matrix = [[\"N\" for _ in range(4)] for _ in range(4)]\n",
        "my_pi = Grid(my_pi_matrix)\n",
        "\n",
        "resulting_policy = policy_iter.run(my_pi, 15, 10)\n",
        "resulting_policy.display()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aetm8L2c7_gK",
        "outputId": "13c1912e-86eb-41fe-eeae-9f2091aaa9c8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['E' 'E' 'E' 'N']\n",
            " ['N' 'N' 'N' 'N']\n",
            " ['N' 'N' 'N' 'W']\n",
            " ['N' 'N' 'N' 'N']]\n"
          ]
        }
      ]
    }
  ]
}